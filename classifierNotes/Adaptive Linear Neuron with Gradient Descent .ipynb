{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapative Linear Neuron\n",
    "Adaline\n",
    "\n",
    "## Adaline\n",
    "\n",
    "This was published a few users after Rosenblatt's perceptron algorithm, by Bernard Widrow. 1960. What makes the Adaline algorithm interesting is because it illustrates the key concepts of defining and minimizing cost functions. This ultimately laid the groundwork for classification such as logistic regression and support vector machines.\n",
    "\n",
    "#### Difference between Adaline and Perceptron\n",
    "\n",
    "*Widrow-Hoff rule* - Adaline weights are updated based on a linear activation function rather than a unit step function like in the perceptron.\n",
    "The linear activation function $ \\phi(z) $ is simply the identity function of the net input so that $ \\phi(w^Tx)=w^Tx $.\n",
    "\n",
    "While the linear activation function is used for learning the weights, a *quantizer* is used to predict the class labels.\n",
    "\n",
    "#### Minimizing cost functions with gradient descent\n",
    "\n",
    "A key ingredient of supervised machine learning is to define an *objective function* that is to be optimized during the learning process. This objective function is often a *cost function* that we want to minimize. In the case of Adaline, we can define the cost function *J* to learn the weights as the **Sum of Squared Errors (SSE)** between the calculated outcome and the true class label. This is the cost function (continuous linear activation function):\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{2}\\sum_i(y^i-\\phi(z^i))^2\n",
    "$$\n",
    "\n",
    "What makes this continous linear activation function have an advantage over the unit step function is that the cost function is differentiable. Since this cost function is convex, we can use *gradient descent* to find the weights that minimize our cost function to classify the samples in the a dataset. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Conclusion of Adapative Linear Neuron\n",
    "\n",
    "* Uses continous predicted values (from the net input) to learn the model coefficients. This is better than the Perceptron because it shows how correct you are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "Quantizer - \n",
    "\n",
    "Cost Function -\n",
    "\n",
    "Sum of Squared Errors -\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "Python Machine Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
