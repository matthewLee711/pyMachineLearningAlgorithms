{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Maching Learning Algorithms for Classification\n",
    "\n",
    "## Early machine learning: Artificial Neurons\n",
    "\n",
    "Warren McCullock and Walter Pitts first concept of simplified brain cell. MCP Neuron.\n",
    " * McCullock and Pitts described such a nerve cell as a simple logic gate with binary outputs; multiple signals arrive at the dendrites, are then integrated into the cell body, and, if the accumulated signal exceeds a certain threshold, an output signal is generated that will be passed on by the axon.\n",
    " \n",
    "Frank Rosenblatt - proposed perceptron learning rule. Algo which learned the optimal weight coefficients that are then multiplied with input features in order to make the decision of whether a neuron fires or not. This can be posed as a binary classification task. 1(positive class), -1(negative).\n",
    "\n",
    "## Perceptron: \n",
    "### Math:\n",
    "We can then define an activation function \u001d",
    "$\\phi(z)$ that takes a linear combination of certain input values x and a corresponding weight vector w , where z is the so-called net input $ z = w_1x_1 + ... + w_mx_m$:\n",
    "\n",
    "$$\n",
    "w =\n",
    "        \\begin{bmatrix}\n",
    "        w_1 \\\\\n",
    "        x_m \\\\\n",
    "        \\end{bmatrix}\n",
    ",\n",
    "x =\n",
    "        \\begin{bmatrix}\n",
    "        x_1 \\\\\n",
    "        x_m \\\\\n",
    "        \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If the activation of a particular sample $x^i$, the output of $\\phi(z)$ is greater than the defined threshold $\\theta$, we predict class 1 and class -1. Otherwise, in the perceptron algorithm, the activation function is a simple *unit step function* \n",
    "\n",
    "AKA \n",
    "\n",
    "*Heaviside step function:* This equation can also be created by taking the derivative of the ramp function.\n",
    "\n",
    "$$\n",
    "\\phi(z)= \n",
    "    \\begin{cases}\n",
    "    1 if z >= \\theta \\\\ \n",
    "    -1 otherwise \\\\\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "OVERALL:\n",
    "\n",
    "For simplicity, we can bring the threshold \u001f to the left side of the equation and\n",
    "define a weight-zero as $w_0 = âˆ’\u001f\\theta$ and $x_0=1$, so that we write **z** in a more compact form. \n",
    "ALSO \n",
    "\n",
    "\n",
    "$$z=w_0x_0 + w_1x_1+...+w_mx_m = w^Tx \\text{ and } \\phi(z)= \n",
    "    \\begin{cases}\n",
    "    1 if z >= \\theta \\\\ \n",
    "    -1 \\text{otherwise} \\\\\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "**Extras:**\n",
    "\n",
    "$\n",
    "z=w_0x_0 + w_1x_1+...+w_mx_m = \\sum_{j=0}^m = w^Tx\n",
    "$\n",
    "Vector dot product\n",
    "\n",
    "Superscript T stands for transpose which can transform a column vector into a row vector \n",
    "http://www.cs.cmu.edu/~zkolter/course/linalg/linalg_notes.pdf.\n",
    "P20 Is important\n",
    "\n",
    "### The overal idea behind MCP neuron and Rosenblatt's trashold percptron model is to mimic how a single neuron in the brain works: it fires or it does not.\n",
    "##### Steps:\n",
    "1. Initialize weights to 0 or small random numbers.\n",
    "2. For each training same $x^i$ perform the following steps:\n",
    "    1. Computer the output value $\\hat{y}$\n",
    "    2. Update the weights\n",
    "\n",
    "Formal EQ for simultaneous update of each $w_j$ in the weight vector **w**:\n",
    "\n",
    "$$w_j:=w_j + {\\Delta}w_j$$\n",
    "\n",
    "The value of ${\\Delta}w_j$ is used to update the weight $w_j$, calculated by perceptron learning rule:\n",
    "\n",
    "$${\\Delta}w_j = {\\eta}(y^i-\\hat{y}^i)x_j^i$$\n",
    "\n",
    "Where **$\\eta$ is the learning rate** (constant between 0.0 and 1.0), **$y^i$ is the true class label of the *i*th training sample** and **$\\hat{y}^i$ is the predicted class label**. It's important to note all weights in the weight vector are being updated **simultaneously** which means we dont recompute $\\hat{y}^i$ before all the weights $\\Delta w_j$ were updated.\n",
    "\n",
    "EX for 2D dataset.\n",
    "\n",
    "$${\\Delta}w_0 = {\\eta}(y^i-output^i)$$\n",
    "$${\\Delta}w_1 = {\\eta}(y^i-output^i)x_1^i$$\n",
    "$${\\Delta}w_2 = {\\eta}(y^i-output^i)x_1^i$$\n",
    "\n",
    "#### Thought experiment:\n",
    "In two scenarios where the perceptron predicts the class label correctly, the weights remain unchanged.\n",
    "\n",
    "$${\\Delta}w_j = {\\eta}(-1^i--1^i)x_j^i = 0$$\n",
    "$${\\Delta}w_j = {\\eta}(1^i-1^i)x_j^i = 0$$\n",
    "\n",
    "However, in the case of a wrong prediction, the weights are being pushed towares the direction of the positive or negative target class respectively:\n",
    "\n",
    "$${\\Delta}w_j = {\\eta}(-1^i--1^i)x_j^i = \\eta{2}x_j^i$$\n",
    "$${\\Delta}w_j = {\\eta}(1^i-1^i)x_j^i = \\eta{-2}x_j^i$$\n",
    "\n",
    "Simple example:\n",
    "\n",
    "## Conclusion of Perceptron\n",
    "* The perceptron recieves the inputs of a sample **x** and combines them with the weights **w** to compute the net input. The net input is then passed on to the activation function (unit step function), which generates a binary output -1 or +1 -- the predicted class label of the smaple. During the learning phase, this output is used to calculate the error of the prediction and update the weights.\n",
    "\n",
    "\n",
    "* The convergence of the perceptron is only guaranteed if the two classes are linearly separable and the learning rate is sufficiently. Epochs can be set which sets a maximum number of passes over the training dataset.\n",
    "\n",
    "* Classifier for binary classification\n",
    "\n",
    "* Uses a linear decision boundary\n",
    "\n",
    "* Can learn iteratively, sample by sample (the Perceptron naturally, and Adaline via stochastic gradient descent)\n",
    "\n",
    "* If you have a neural network (aka a multilayer perceptron) with only an input and an output layer and with no activation function, that is exactly equal to linear regression.\n",
    "http://cs.stackexchange.com/questions/28597/difference-between-multilayer-perceptron-and-linear-regression\n",
    "\n",
    "## Classification machine learning algorithms:\n",
    "Perceptron\n",
    "\n",
    "Adaptive linear neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Index\n",
    "\n",
    "Activation function - a function that defines the output based on a given input or set of inputs. For example, a logistic regression can be created by inserting a your equation of choice into a Sigmoid function.\n",
    "\n",
    "Unit step function - (Heaviside step function) denoted by $\\theta$ is a discontinous function function whose value is zero for negative arguments and one for positive arguments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
